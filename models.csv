model_idx,code,feature_vectors
1,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(32 * (input_size // 2), 128)  # Adjust for the reduced dimension after pooling\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)  # Add regularization with dropout\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)  # Apply dropout\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n""",
2,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n""",
3,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n""",
4,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)  # Wide first layer\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n\n""",
5,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 16)\n        self.fc4 = nn.Linear(16, num_classes)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.tanh(x)\n        x = self.fc2(x)\n        x = self.tanh(x)\n        x = self.fc3(x)\n        x = self.tanh(x)\n        x = self.fc4(x)\n        return x\n\n""",
6,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.fc3 = nn.Linear(256, 128)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.fc4 = nn.Linear(128, 64)\n        self.fc5 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.4)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = self.fc3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n        \n        x = self.fc4(x)\n        x = self.fc5(x)\n        return x\n\n""",
7,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 32)  \n        self.fc2 = nn.Linear(32, num_classes) \n        self.relu = nn.ReLU()                \n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)  # Apply ReLU after the first layer\n        x = self.fc2(x)             \n        return x\n\n""",
8,"""\nclass ResidualBlock(nn.Module):\n    def __init__(self, input_size):\n        super(ResidualBlock, self).__init__()\n        self.fc1 = nn.Linear(input_size, input_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(input_size, input_size)\n\n    def forward(self, x):\n        residual = x\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        x += residual  # Skip connection\n        return x\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.res1 = ResidualBlock(256)\n        self.res2 = ResidualBlock(256)\n        self.fc2 = nn.Linear(256, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.res1(x)\n        x = self.res2(x)\n        x = self.fc2(x)\n        return x\n""",
9,"""\n\nimport torch.nn.functional as F\n\nclass BottleneckResidualBlock(nn.Module):\n    def __init__(self, in_features, bottleneck_features=16):\n        super(BottleneckResidualBlock, self).__init__()\n        self.fc1 = nn.Linear(in_features, bottleneck_features) \n        self.fc2 = nn.Linear(bottleneck_features, bottleneck_features) \n        self.fc3 = nn.Linear(bottleneck_features, in_features) \n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        x += residual  # Skip connection\n        return F.relu(x)  # Apply activation to the output\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.res1 = BottleneckResidualBlock(256, 64)\n        self.res2 = BottleneckResidualBlock(256, 64)\n        self.fc2 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.res1(x)\n        x = self.res2(x)\n        x = self.fc2(x)\n        return x\n\n""",
10,"""\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        # Fixed parameters\n        self.hidden_size = 64  # Number of hidden neurons\n        self.num_layers = 2    # Number of RNN layers\n        \n        # RNN layer\n        self.rnn = nn.RNN(input_size, self.hidden_size, self.num_layers, batch_first=True)\n        \n        # Fully connected output layer\n        self.fc = nn.Linear(self.hidden_size, num_classes)\n\n    def forward(self, x):\n        # Reshape input to add a sequence length of 1\n        x = x.unsqueeze(1)  # Shape: (batch_size, seq_length=1, input_size)\n        \n        # Initialize hidden state (h0) with zeros\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n\n        # Forward propagate RNN\n        out, _ = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n\n        # Take the output from the last time step\n        out = out[:, -1, :]  # Get last time step's output\n\n        # Pass through the fully connected layer\n        out = self.fc(out)\n        return out\n\n""",
11,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.fc3 = nn.Linear(256, 128)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.fc4 = nn.Linear(128, 64)\n        self.fc5 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.4)\n\n    def forward(self, x):\n        x = self.relu(self.bn1(self.fc1(x)))\n        x = self.dropout(x)\n        x = self.relu(self.bn2(self.fc2(x)))\n        x = self.dropout(x)\n        x = self.relu(self.bn3(self.fc3(x)))\n        x = self.fc4(x)\n        x = self.fc5(x)\n        return x\n\n""",
12,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.05)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.leaky_relu(x)\n        \n        x = self.fc2(x)\n        x = self.leaky_relu(x)\n        \n        x = self.fc3(x)\n        return x\n\n""",
13,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.tanh(x)\n        \n        x = self.fc2(x)\n        x = self.tanh(x)\n        \n        x = self.fc3(x)\n        return x\n\n""",
14,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        \n        x = self.fc2(x)\n        x = self.relu(x)\n        \n        x = self.fc3(x)\n        return x\n\n""",
15,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.2)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        \n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = self.fc3(x)\n        x = self.fc4(x)\n        return x\n\n""",
16,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(input_size, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x)\n        x = torch.max(x1, x2)\n        x = self.fc3(x)\n        return x\n\n""",
17,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, input_size)\n        self.fc2 = nn.Linear(input_size, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = x +  residual\n        \n        x = self.fc2(x)\n        x = self.relu(x)\n        \n        x = self.fc3(x)\n        return x\n\n""",
18,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        \n        x = self.fc2(x)\n        x = self.tanh(x)\n        \n        x = self.fc3(x)\n        return x\n""",
19,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout1 = nn.Dropout(p=0.3)\n        self.dropout2 = nn.Dropout(p=0.5)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout1(x)\n        \n        x = self.fc2(x)\n        x = self.relu(x)\n        \n        x = self.fc3(x)\n        x = self.dropout2(x)\n        \n        x = self.fc4(x)\n        return x\n\n""",
20,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, 64)\n        self.fc5 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        \n        x = self.fc2(x)\n        x = self.relu(x)\n        \n        x = self.fc3(x)\n        x = self.relu(x)\n        \n        x = self.fc4(x)\n        x = self.fc5(x)\n        return x\n\n""",
21,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, num_classes),\n        )\n\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\n""",
22,"""\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 32)\n        self.fc2 = nn.Linear(32, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n""",
23,"""\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.relu = nn.ReLU()\n        \n        dummy_input = torch.zeros(1, 1, input_size)  # Batch size 1, channel 1, input length\n        with torch.no_grad():\n            dummy_output = self.pool(self.relu(self.conv1(dummy_input)))\n            flattened_size = dummy_output.numel()  # Total number of elements in the flattened output\n        \n        self.fc1 = nn.Linear(flattened_size, 64)\n        self.fc2 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # Add channel dimension\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n""",
24,"""\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.relu = nn.ReLU()\n        \n        # Dynamically calculate the flattened size after conv and pool\n        dummy_input = torch.zeros(1, 1, input_size)  # Batch size 1, channel 1, input length\n        with torch.no_grad():\n            dummy_output = self.pool(self.relu(self.conv2(self.relu(self.conv1(dummy_input)))))\n            flattened_size = dummy_output.numel()  # Total number of elements in the flattened output\n        \n        self.fc1 = nn.Linear(flattened_size, 64)  # Use the calculated size\n        self.fc2 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # Add channel dimension\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n\n""",
25,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.fc2 = nn.Linear(128, 64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n\n""",
26,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 16)\n        self.bn1 = nn.BatchNorm1d(16)\n        self.fc2 = nn.Linear(16, 64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n\n""",
27,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.shortcut = nn.Linear(input_size, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = self.shortcut(x)  # Skip connection\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x) + residual  # Add skip connection\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n\n""",
28,"""\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.fc3 = nn.Linear(256, num_classes)\n        self.dropout = nn.Dropout(0.5)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n\n""",
29,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.shortcut = nn.Linear(input_size, 64)  # Residual connection\n        self.fc4 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = self.shortcut(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x) + residual\n        x = self.relu(x)\n        x = self.fc4(x)\n        return x\n\n""",
30,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 8)  \n        self.fc2 = nn.Linear(8, num_classes) \n        self.relu = nn.ReLU()                \n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)  # Apply ReLU after the first layer\n        x = self.fc2(x)             \n        return x\n\n""",
31,"""\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        layers = []\n        in_features = input_size\n        layers.append(nn.Linear(in_features, 256))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(256, 128))\n        layers.append(nn.Linear(128, 128))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(128, 256))\n        layers.append(nn.Linear(256, num_classes))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n""",
32,"""\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        layers = []\n        in_features = input_size\n        layers.append(nn.Linear(in_features, 64))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(64, 32))\n        layers.append(nn.Linear(32, 64))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(64, 128))\n        layers.append(nn.Linear(128, num_classes))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n""",
33,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size, 64)\n        self.layer2 = ResidualBlock(64, 128)\n        self.layer3 = ResidualBlock(128, 256)\n        self.layer4 = ResidualBlock(256, 512)\n        self.fc = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return self.fc(x)\n        \nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, out_features),\n            nn.ReLU(),\n            nn.Linear(out_features, out_features)\n        )\n        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n\n    def forward(self, x):\n        return nn.ReLU()(self.fc(x) + self.shortcut(x))\n\n""",
34,"""\nimport torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, out_features),\n            nn.ReLU(),\n            nn.Linear(out_features, out_features)\n        )\n        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n\n    def forward(self, x):\n        return nn.ReLU()(self.fc(x) + self.shortcut(x))\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size, 64)\n        self.layer2 = ResidualBlock(64, 128)\n        self.fc = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return self.fc(x)\n\n""",
35,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size, 128)\n        self.layer2 = ResidualBlock(128, 256)\n        self.layer3 = ResidualBlock(256, 512)\n        self.fc = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return self.fc(x)\n\n""",
36,"""\nclass BottleneckBlock(nn.Module):\n    def __init__(self, in_features, bottleneck_size, out_features):\n        super(BottleneckBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, bottleneck_size),\n            nn.ReLU(),\n            nn.Linear(bottleneck_size, out_features)\n        )\n        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n\n    def forward(self, x):\n        return nn.ReLU()(self.fc(x) + self.shortcut(x))\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = BottleneckBlock(input_size, 32, 128)\n        self.layer2 = BottleneckBlock(128, 64, 256)\n        self.layer3 = BottleneckBlock(256, 128, 512)\n        self.fc = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return self.fc(x)\n\n""",
37,"""\nclass ResidualDenseBlock(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ResidualDenseBlock, self).__init__()\n        self.fc1 = nn.Linear(in_features, out_features)\n        self.fc2 = nn.Linear(out_features, out_features)\n        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n\n    def forward(self, x):\n        return nn.ReLU()(self.fc2(nn.ReLU()(self.fc1(x))) + self.shortcut(x))\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualDenseBlock(input_size, 128)\n        self.layer2 = ResidualDenseBlock(128, 256)\n        self.fc = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return self.fc(x)\n\n""",
38,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size, 128)\n        self.dropout = nn.Dropout(0.5)\n        self.layer2 = ResidualBlock(128, 256)\n        self.fc = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.dropout(x)\n        x = self.layer2(x)\n        return self.fc(x)\n\n""",
39,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size, 128)\n        self.norm = nn.LayerNorm(128)\n        self.layer2 = ResidualBlock(128, 256)\n        self.fc = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.norm(self.layer1(x))\n        x = self.layer2(x)\n        return self.fc(x)\n\n""",
40,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size, 32)\n        self.layer2 = ResidualBlock(32, 64)\n        self.fc = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return self.fc(x)\n\n""",
41,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer_1 = nn.Linear(input_size, 256)\n        self.layer_3 = nn.Linear(256, 128)\n        self.layer_4 = nn.Linear(128, 64)\n        self.final_layer = nn.Linear(64, num_classes)\n        self.activation = nn.ReLU()\n\n    def forward(self, input_data):\n        out = self.layer_1(input_data)\n        out = self.activation(out)\n        \n        out = self.layer_3(out)\n        out = self.activation(out)\n        \n        out = self.layer_4(out)\n        out = self.final_layer(out)\n        return out\n\n""",
42,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.fc2 = nn.BatchNorm1d(512)\n        self.fc3 = nn.Linear(512, 256)\n        self.fc4 = nn.LayerNorm(256)\n        self.fc5 = nn.Linear(256, 128)\n        self.fc6 = nn.BatchNorm1d(128)\n        self.fc7 = nn.Linear(128, 64)\n        self.fc8 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.4)\n\n    def forward(self, x):\n        x = self.relu(self.fc2(self.fc1(x)))\n        x = self.dropout(x)\n        x = self.relu(self.fc4(self.fc3(x)))\n        x = self.dropout(x)\n        x = self.relu(self.fc6(self.fc5(x)))\n        x = self.fc7(x)\n        x = self.fc8(x)\n        return x\n""",
43,"""\nclass Model(nn.Module):  # Model with dropout regularization\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 32)\n        self.fc4 = nn.Linear(32, num_classes)\n        \n        self.activation_function = nn.Tanh()\n\n    def forward(self, data):\n        output = self.fc1(data)\n        output = self.activation_function(output)\n        \n        output = self.fc2(output)\n        output = self.activation_function(output)\n        \n        output = self.activation_function(self.fc3(output))\n\n        output = self.fc4(output)\n        return output\n""",
44,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        # A random comment here trying to confuse codeBERT\n        self.fc1 = nn.Linear(input_size, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        \n        x = self.fc2(x)\n        x = self.relu(x)\n        \n        x = self.fc3(x)\n        return x\n\n""",
45,"""\nclass Model(nn.Module):  # Model with dropout regularization\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, num_classes)     \n        self.activation_function = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.2)\n    # I will try to finish my C25 lecture today as well. I should not spend all my time on this project\n    def forward(self, data):\n        output = self.fc1(data)\n        output = self.activation_function(output)\n        \n        output = self.fc2(output)\n        output = self.activation_function(output)\n        output = self.dropout(output)\n        \n        output = self.fc3(output)\n        output = self.fc4(output)\n        return output\n""",
46,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(input_size, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n\n        \n\n\n\n\n\n        \n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x)\n        x = torch.max(x1, x2)\n        x = self.fc3(x)\n        return x\n\n""",
47,"""\nclass Model(nn.Module):  # Defines the main model class\n    def __init__(self, input_size, num_classes):  # Constructor\n        super(Model, self).__init__()  # Call parent constructor\n\n        # Layer Definitions\n        self.hidden_layer1 = nn.Linear(input_size, input_size)  # First fully connected layer\n        self.hidden_layer2 = nn.Linear(input_size, 128)  # Second fully connected layer\n        self.final_layer = nn.Linear(128, num_classes)  # Output layer\n\n        # Activation Function\n        self.activation_fn = nn.ReLU()  # Using ReLU activation\n\n    def forward(self, inputs):  # Forward pass logic\n        # Preserve the residual for skip connection\n        skip_connection = inputs  \n\n        # First Layer\n        result = self.hidden_layer1(inputs)  # Apply first linear layer\n        result = self.activation_fn(result)  # Apply ReLU activation\n        result = result +  skip_connection  # Add residual (skip connection)\n\n        # Second Layer\n        result = self.hidden_layer2(result)  # Apply second linear layer\n        result = self.activation_fn(result)  # Apply ReLU activation\n\n        # Final Output\n        result = self.final_layer(result)  # Apply final layer for output\n        return result  # Return output\n""",
48,"""\nclass Model(nn.Module):  # Changed class name for added confusion\n    def __init__(self, input_size, num_classes):  # Updated parameter names\n        super(Model, self).__init__()  # Superclass initialization\n\n        # Initialize layers with descriptive names\n        self.input_layer = nn.Linear(input_size, 128)  # First fully connected layer\n        self.hidden_layer = nn.Linear(128, 64)  # Hidden layer\n        self.output_layer = nn.Linear(64, num_classes)  # Final layer\n\n        # Activation Functions\n        self.activation_relu = nn.ReLU()  # ReLU activation\n        self.activation_tanh = nn.Tanh()  # Tanh activation\n\n    def forward(self, inputs):  # Updated parameter name\n        # First Layer\n        temp_result = self.input_layer(inputs)  # Linear transformation\n        temp_result = self.activation_relu(temp_result)  # ReLU activation\n\n        # Second Layer\n        temp_result = self.hidden_layer(temp_result)  # Linear transformation\n        temp_result = self.activation_tanh(temp_result)  # Tanh activation\n\n        # Output Layer\n        output = self.output_layer(temp_result)  # Final linear transformation\n\n        return output  # Return final output\n""",
49,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size)\n        self.layer2 = ResidualBlock(input_size)\n        self.fc = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return self.fc(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, input_size):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, input_size),\n            nn.ReLU(),\n            nn.Linear(input_size, input_size)\n        )\n\n    def forward(self, x):\n        return torch.relu(self.fc(x) + x)\n\n""",
50,"""\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.hidden = nn.Linear(input_size, 64)  \n        self.output_layer = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.hidden(x))\n        return self.output_layer(x)\n\n""",
51,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\n""",
52,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 256)\n        self.fc3 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\n""",
53,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.layer1 = nn.Linear(input_size, 128)\n        self.batch_norm = nn.BatchNorm1d(128)\n        self.layer2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.batch_norm(torch.relu(self.layer1(x)))\n        return self.layer2(x)\n\n""",
54,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.dense1 = nn.Linear(input_size, 256)\n        self.dense2 = nn.Linear(256, 512)\n        self.drop = nn.Dropout(0.3)\n        self.dense3 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.dense1(x))\n        x = self.drop(torch.relu(self.dense2(x)))\n        return self.dense3(x)\n\n""",
55,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.l1 = nn.Linear(input_size, 64)\n        self.l2 = nn.Linear(64, 128)\n        self.l3 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = torch.selu(self.l1(x))\n        x = torch.selu(self.l2(x))\n        return self.l3(x)\n\n""",
56,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.linear1 = nn.Linear(input_size, 128)\n        self.linear2 = nn.Linear(128, 64)\n        self.linear3 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = torch.nn.functional.leaky_relu(self.linear1(x), negative_slope=0.02)\n        x = torch.nn.functional.leaky_relu(self.linear2(x), negative_slope=0.02)\n        return self.linear3(x)\n\n""",
57,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.fc1 = nn.Linear(input_size, 128)\n        self.norm = nn.LayerNorm(128)\n        self.fc2 = nn.Linear(128, 256)\n        self.fc3 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.norm(torch.relu(self.fc1(x)))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\n""",
58,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = torch.sigmoid(self.fc1(x))  # Swish activation\n        return self.fc2(x)\n\n""",
59,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.hidden_layer = nn.Linear(input_size, 128)\n        self.output_layer = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = torch.tanh(self.hidden_layer(x))\n        return self.output_layer(x)\n\n""",
60,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.l1 = nn.Linear(input_size, 64)\n        self.l2 = nn.Linear(64, 64)\n        self.l3 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        identity = x\n        x = torch.relu(self.l1(x))\n        x = x + self.l2(x)  # Residual connection\n        return self.l3(x)\n\n""",
61,"""\nimport torch\nimport torch.nn as nn\n\n\n# Standard MLP but with extra spaces and unnecessary comments\nclass Model(nn.Module):\n    \n    def __init__( self ,input_size , num_classes ):  \n        super( Model , self ).__init__()  \n        \n        # First hidden layer\n        self.layer_one = nn.Linear( input_size , 128 )  \n\n        # Output layer\n        self.final_output = nn.Linear( 128 , num_classes )  \n        \n\n    def forward( self , x ) :  \n\n        # Apply activation function  \n        x = torch.relu(self.layer_one(x))  \n        \n        # Final output  \n        return self.final_output(x)  \n\n\n""",
62,"""\nfrom torch import nn\nimport torch\nclass Model ( nn.Module ):    \n\n    def __init__ ( self , input_size , num_classes ):   \n        super( ) . __init__( )    \n\n        self.layer1 = nn.Linear( input_size, 128 )   \n\n        # Intermediate hidden layer   \n        self.layer2 = nn.Linear( 128 , 256 )    \n\n        # Output layer    \n        self.layer3 = nn.Linear( 256 , num_classes )    \n\n        \n    def forward ( self , x ):  \n\n        # Pass input through first layer\n        x = torch . relu ( self.layer1 ( x ) )    \n\n\n        # Apply second layer  \n        x = torch . relu ( self.layer2 ( x ) )    \n\n        # Output layer \n        return self.layer3 ( x )  \n""",
63,"""\nclass Model(nn.Module):\n    \n    \n    def __init__(self, input_size, num_classes):\n        \n        \n        super(Model, self).__init__()\n        \n        self.hidden = nn.Linear(input_size, 64)  \n        \n        self.final_layer = nn.Linear(64, num_classes)  \n        \n\n\n    def forward(self, x):  \n        \n        # Apply first layer activation\n        x = torch.relu(\n            self.hidden(x)\n        )  \n        \n        \n        return self.final_layer(x)  \n\n""",
64,"""\nclass Model(nn.Module):\n    \n    def __init__(self, input_size, num_classes):\n\n        super(Model, self).__init__()\n\n        self.hidden_layer = nn.Linear(input_size, 64)\n        \n        # Output layer\n        self.output_layer = nn.Linear(64, num_classes)\n\n        \n    def forward(self, x):\n\n        x = torch.relu(\n                (self.hidden_layer(x))\n            )  \n\n        \n        return (\n            self.output_layer(x)\n        )\n\n""",
65,"""\nclass Model(nn.Module):\n\n    def __init__( self, input_size, num_classes ) :\n\n        super(Model, self).__init__()\n\n        # First fully connected layer\n        self.dense1 = nn.Linear(input_size, 128)\n        \n\n        self.dense2 = nn.Linear(128, num_classes)\n\n        \n    # Forward pass\n    def forward( self, x ) :\n\n        x = torch.relu( self.dense1( x ) ) \n\n        return self.dense2( x )  \n\n""",
66,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.layer1 = nn.Linear( input_size, 128 ) \n        self.layer2 = nn.Linear( 128, 256 ) \n        self.layer3 = nn.Linear( 256, num_classes ) \n\n\n    def forward(self, x):\n        \n        x = torch.relu(\n                    self.layer1(x)\n                )  \n        \n        x = torch.relu(  \n            (self.layer2(x))  \n        )  \n\n        return (\n            self.layer3( x )  \n        )  \n\n""",
67,"""\nclass Model(nn.Module):\n    \n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        # Hidden layer\n        self.fc1 = nn.Linear(input_size, 128)\n\n        # Normalization layer\n        self.norm = nn.LayerNorm(128)\n\n        self.fc2 = nn.Linear(128, num_classes)\n\n\n    def forward(self, x):\n\n        # Apply first layer + normalization\n        x = self.norm( torch.relu( self.fc1(x) ) )\n\n        return self.fc2( x )\n\n""",
68,"""\nclass Model(nn.Module):\n\n    \n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.hidden = nn.Linear(input_size, 64)\n        \n        self.output_layer = nn.Linear(64, num_classes)\n        \n\n    \n    def forward(self, x):\n\n        x = torch.relu( self.hidden( x ) )\n\n        return self.output_layer(x)\n\n""",
69,"""\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.l1 = nn.Linear(input_size, 64)\n        self.l2 = nn.Linear(64, 64)\n        self.l3 = nn.Linear(64, num_classes)\n\n\n    def forward(self, x):\n\n        identity = x  \n        \n        x = torch.relu( self.l1( x ) )  \n\n        x = (  \n            x + self.l2( x )  \n        )  \n\n        return self.l3(x)\n\n""",
70,"""\nclass Model(nn.Module):\n    \n    \n\n    def __init__(self, input_size, num_classes):\n\n        super(Model, self).__init__()\n\n        self.l1 = nn.Linear(input_size, 64)\n        self.l2 = nn.Linear(64, 128)\n        self.l3 = nn.Linear(128, num_classes)\n\n\n    def forward(self, x):\n\n\n        x = torch.selu(\n            self.l1( x )\n        )\n\n        \n        x = torch.selu( self.l2( x ) )  \n\n        return self.l3(x)\n\n""",
71,"""\nclass Model(nn.Module):\n\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.layer1 = nn.Linear(input_size, 64)\n        \n        self.layer2 = nn.Linear(64, num_classes)\n\n\n    \n    def forward(self, x):\n\n        with torch.no_grad():  # Unnecessary but still functional\n            temp = x\n\n        x = torch.relu(self.layer1(temp))\n\n        return self.layer2(x)\n\n""",
72,"""\nclass Model(nn.Module):\n\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.layer1 = nn.Linear(input_size, 128)\n        \n        self.layer2 = nn.Linear(128, num_classes)\n\n\n    \n    def forward(self, x):\n\n        with torch.no_grad():  # Unnecessary but still functional\n            temp = x\n\n        x = torch.relu(self.layer1(temp))\n\n        return self.layer2(x)\n\n""",
73,"""\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.hidden1 = nn.Linear(input_size, 64)\n        self.hidden2 = nn.Linear(64, 128)\n        self.hidden3 = nn.Linear(128, 256)  # Never used\n\n        self.final_layer = nn.Linear(64, num_classes)\n\n\n    def forward(self, x):\n\n        x = torch.relu(self.hidden1(x))\n\n        return self.final_layer(x)\n\n""",
74,"""\n\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.l1 = nn.Linear(input_size, input_size)\n        self.l2 = nn.Linear(input_size, num_classes)\n\n    \n    def forward(self, x):\n\n        x = self.l1(x)  \n        x = torch.relu(x)  \n\n        x = self.l1(x)  # Unnecessary reuse of `l1`\n\n        return torch.relu(self.l2(x))\n""",
75,"""\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    \n    def forward(self, x):\n\n        x = torch.relu(self.fc1(x))\n\n        x = sum([self.fc2(x) for _ in range(1)])  # Pointless list comprehension\n\n        return x\n\n""",
76,"""\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.layer1 = nn.Linear(input_size, 256)\n        self.identity = nn.Identity()  # Does absolutely nothing\n        self.output_layer = nn.Linear(256, num_classes)\n\n\n    def forward(self, x):\n\n        x = torch.relu(self.layer1(x))\n\n        x = self.identity(x)  # Identity layer doesn\u2019t affect anything\n\n        return self.output_layer(x)\n\n""",
77,"""\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # Ensure the input has (batch_size, channels, sequence_length)\n        x = torch.relu(self.conv1(x))\n        x = x.mean(dim=-1)  # Global Average Pooling\n        return self.fc(x)\n\n\n""",
78,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n""",
79,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n        self.fc = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n""",
80,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool1d(kernel_size=2)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = self.pool(x)\n        x = torch.relu(self.conv2(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n""",
81,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm1d(32)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = torch.relu(self.bn2(self.conv2(x)))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n""",
82,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = self.dropout(x.mean(dim=-1))\n        return self.fc(x)\n\n""",
83,"""\nclass Model(nn.Module):\n    def __init__(self,input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n""",
84,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n        self.fc = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        identity = x\n        x = torch.relu(self.conv1(x))\n        x = self.conv2(x) + identity  # Residual connection\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n""",
85,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n        self.fc = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        identity = x\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x)) + identity\n        x = self.conv3(x)\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n""",
86,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.norm = nn.LayerNorm(64)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))  # Shape: (batch_size, 64, seq_length)\n        x = torch.relu(self.conv2(x))             # Shape: (batch_size, 32, seq_length)\n        x = x.mean(dim=-1)                       # Global average pooling\n        return self.fc(x)\n\n\n""",
87,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=2, dilation=2)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=2, dilation=2)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n""",
88,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=4, dilation=4)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=4, dilation=4)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n""",
89,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1) \n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n""",
90,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(128, 64, kernel_size=3, padding=1, groups=4)\n        self.conv3 = nn.Conv1d(64, 32, kernel_size=3, padding=1, groups=4)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n""",
91,"""\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=2, dilation=2)\n        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=2, dilation=2, groups=2)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n""",
92,"""\nclass Model ( nn.Module ) :  \n      \n    def __init__ ( self , input_size , num_classes ):   \n        super( Model , self ) . __init__ ( )    \n\n        # First Convolution Layer\n        self.conv1 = nn.Conv1d( 1 , 64 , kernel_size = 3 , padding = 1 )  \n\n\n        # Second Convolution Layer\n        self.conv2 = nn.Conv1d( 64 , 32 , kernel_size = 3 , padding = 1 )    \n\n        self.fc = nn.Linear ( 32 , num_classes )    \n\n        \n    def forward ( self , x ) :  \n        x = x.unsqueeze(1)\n        x = torch . relu ( ( self.conv1 ( x ) ) )    \n\n\n        x = torch . relu ( ( self.conv2 ( x ) ) )    \n\n        return ( ( self.fc ( x.mean ( dim = -1 ) ) ) )  \n\n""",
93,"""\nclass Model(nn.Module):\n\n\n\n    def __init__(self, input_size, num_classes):\n\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n\n\n        self.fc = nn.Linear(32, num_classes)\n\n\n    \n    def forward(self, x):\n\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n\n        temp_var = x  # Temporary variable that is unnecessary\n\n        x = torch.relu(self.conv2(temp_var))\n\n        return self.fc(x.mean(dim=-1))\n\n""",
94,"""\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.layer_one = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n        self.layer_two = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n        self.layer_three = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n\n        self.fc = nn.Linear(32, num_classes)\n\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(\n                (self.layer_one(x))\n        )  \n\n        x = torch.relu(\n                (self.layer_two(x))\n        )  \n\n        x = torch.relu(\n                (self.layer_three(x))\n        )  \n\n        return self.fc(\n            x.mean(dim=-1)\n        )\n\n""",
95,"""\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n\n        self.fc = nn.Linear(64, num_classes)\n\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n\n\n        # Useless conditional statement that does nothing\n        if True:\n            x = torch.relu(self.conv2(x))\n\n        return self.fc(x.mean(dim=-1))\n\n""",
96,"""\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n\n        self.fc = nn.Linear(32, num_classes)\n\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        with torch.no_grad():  # Doesn't serve any purpose here\n            temp_x = x\n\n        x = torch.relu(self.conv1(temp_x))\n\n        x = torch.relu(self.conv2(x))\n\n        return self.fc(x.mean(dim=-1))\n\n""",
97,"""\nimport torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, input_size):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, input_size),\n            nn.ReLU(),\n            nn.Linear(input_size, input_size)\n        )\n\n    def forward(self, x):\n        return torch.relu(self.fc(x) + x)\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.block1 = ResidualBlock(input_size)\n        self.fc = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        return self.fc(x)\n\n""",
98,"""\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.block1 = ResidualBlock(input_size)\n        self.block2 = ResidualBlock(input_size)\n        self.fc = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        return self.fc(x)\n\n        import torch\n        \nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, in_features),\n            nn.ReLU(),\n            nn.Linear(in_features, in_features)\n        )\n\n    def forward(self, x):\n        return torch.relu(self.fc(x) + x)\n""",
99,"""\nimport torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, input_size):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, input_size),\n            nn.ReLU(),\n            nn.Linear(input_size, input_size)\n        )\n\n    def forward(self, x):\n        return torch.relu(self.fc(x) + x)\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.block1 = ResidualBlock(input_size)\n        self.fc1 = nn.Linear(input_size, 256)\n        self.block2 = ResidualBlock(256)\n        self.fc = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.fc1(x)\n        x = self.block2(x)\n        return self.fc(x)\n\n""",
100,"""\nclass BottleneckBlock(nn.Module):\n    def __init__(self, in_features, bottleneck_size):\n        super(BottleneckBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, bottleneck_size),\n            nn.ReLU(),\n            nn.Linear(bottleneck_size, in_features)\n        )\n\n    def forward(self, x):\n        return torch.relu(self.fc(x) + x)\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.block1 = BottleneckBlock(input_size, 64)\n        self.block2 = BottleneckBlock(input_size, 128)\n        self.fc = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        return self.fc(x)\n\n""",
101,"""\nimport torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, in_features),\n            nn.ReLU(),\n            nn.Linear(in_features, in_features)\n        )\n\n    def forward(self, x):\n        return torch.relu(self.fc(x) + x)\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.block1 = ResidualBlock(input_size)\n        self.block2 = ResidualBlock(input_size)\n        self.fc = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        x = torch.selu(self.block1(x))\n        x = torch.selu(self.block2(x))\n        return self.fc(x)\n\n""",
