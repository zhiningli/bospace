[{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ac4"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(32 * (input_size // 2), 128)  # Adjust for the reduced dimension after pooling\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)  # Add regularization with dropout\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)  # Apply dropout\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n","model_idx":1},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ac5"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n","model_idx":2},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ac6"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n","model_idx":3},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ac7"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)  # Wide first layer\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n\n","model_idx":4},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ac8"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 16)\n        self.fc4 = nn.Linear(16, num_classes)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.tanh(x)\n        x = self.fc2(x)\n        x = self.tanh(x)\n        x = self.fc3(x)\n        x = self.tanh(x)\n        x = self.fc4(x)\n        return x\n\n","model_idx":5},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ac9"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.fc3 = nn.Linear(256, 128)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.fc4 = nn.Linear(128, 64)\n        self.fc5 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.4)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = self.fc3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n        \n        x = self.fc4(x)\n        x = self.fc5(x)\n        return x\n\n","model_idx":6},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9aca"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 32)  \n        self.fc2 = nn.Linear(32, num_classes) \n        self.relu = nn.ReLU()                \n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)  # Apply ReLU after the first layer\n        x = self.fc2(x)             \n        return x\n\n","model_idx":7},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9acb"},"model_str":"\nclass ResidualBlock(nn.Module):\n    def __init__(self, input_size):\n        super(ResidualBlock, self).__init__()\n        self.fc1 = nn.Linear(input_size, input_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(input_size, input_size)\n\n    def forward(self, x):\n        residual = x\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        x += residual  # Skip connection\n        return x\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.res1 = ResidualBlock(256)\n        self.res2 = ResidualBlock(256)\n        self.fc2 = nn.Linear(256, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.res1(x)\n        x = self.res2(x)\n        x = self.fc2(x)\n        return x\n","model_idx":8},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9acc"},"model_str":"\n\nimport torch.nn.functional as F\n\nclass BottleneckResidualBlock(nn.Module):\n    def __init__(self, in_features, bottleneck_features=16):\n        super(BottleneckResidualBlock, self).__init__()\n        self.fc1 = nn.Linear(in_features, bottleneck_features) \n        self.fc2 = nn.Linear(bottleneck_features, bottleneck_features) \n        self.fc3 = nn.Linear(bottleneck_features, in_features) \n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        x += residual  # Skip connection\n        return F.relu(x)  # Apply activation to the output\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.res1 = BottleneckResidualBlock(256, 64)\n        self.res2 = BottleneckResidualBlock(256, 64)\n        self.fc2 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.res1(x)\n        x = self.res2(x)\n        x = self.fc2(x)\n        return x\n\n","model_idx":9},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9acd"},"model_str":"\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        # Fixed parameters\n        self.hidden_size = 64  # Number of hidden neurons\n        self.num_layers = 2    # Number of RNN layers\n        \n        # RNN layer\n        self.rnn = nn.RNN(input_size, self.hidden_size, self.num_layers, batch_first=True)\n        \n        # Fully connected output layer\n        self.fc = nn.Linear(self.hidden_size, num_classes)\n\n    def forward(self, x):\n        # Reshape input to add a sequence length of 1\n        x = x.unsqueeze(1)  # Shape: (batch_size, seq_length=1, input_size)\n        \n        # Initialize hidden state (h0) with zeros\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n\n        # Forward propagate RNN\n        out, _ = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n\n        # Take the output from the last time step\n        out = out[:, -1, :]  # Get last time step's output\n\n        # Pass through the fully connected layer\n        out = self.fc(out)\n        return out\n\n","model_idx":10},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ace"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.fc3 = nn.Linear(256, 128)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.fc4 = nn.Linear(128, 64)\n        self.fc5 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.4)\n\n    def forward(self, x):\n        x = self.relu(self.bn1(self.fc1(x)))\n        x = self.dropout(x)\n        x = self.relu(self.bn2(self.fc2(x)))\n        x = self.dropout(x)\n        x = self.relu(self.bn3(self.fc3(x)))\n        x = self.fc4(x)\n        x = self.fc5(x)\n        return x\n\n","model_idx":11},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9acf"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.05)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.leaky_relu(x)\n        \n        x = self.fc2(x)\n        x = self.leaky_relu(x)\n        \n        x = self.fc3(x)\n        return x\n\n","model_idx":12},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ad0"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.tanh(x)\n        \n        x = self.fc2(x)\n        x = self.tanh(x)\n        \n        x = self.fc3(x)\n        return x\n\n","model_idx":13},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ad1"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        \n        x = self.fc2(x)\n        x = self.relu(x)\n        \n        x = self.fc3(x)\n        return x\n\n","model_idx":14},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ad2"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.2)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        \n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = self.fc3(x)\n        x = self.fc4(x)\n        return x\n\n","model_idx":15},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ad3"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(input_size, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x)\n        x = torch.max(x1, x2)\n        x = self.fc3(x)\n        return x\n\n","model_idx":16},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ad4"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, input_size)\n        self.fc2 = nn.Linear(input_size, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = x +  residual\n        \n        x = self.fc2(x)\n        x = self.relu(x)\n        \n        x = self.fc3(x)\n        return x\n\n","model_idx":17},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ad5"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        \n        x = self.fc2(x)\n        x = self.tanh(x)\n        \n        x = self.fc3(x)\n        return x\n","model_idx":18},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ad6"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout1 = nn.Dropout(p=0.3)\n        self.dropout2 = nn.Dropout(p=0.5)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout1(x)\n        \n        x = self.fc2(x)\n        x = self.relu(x)\n        \n        x = self.fc3(x)\n        x = self.dropout2(x)\n        \n        x = self.fc4(x)\n        return x\n\n","model_idx":19},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ad7"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, 64)\n        self.fc5 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        \n        x = self.fc2(x)\n        x = self.relu(x)\n        \n        x = self.fc3(x)\n        x = self.relu(x)\n        \n        x = self.fc4(x)\n        x = self.fc5(x)\n        return x\n\n","model_idx":20},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ad8"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, num_classes),\n        )\n\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\n","model_idx":21},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ad9"},"model_str":"\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 32)\n        self.fc2 = nn.Linear(32, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n","model_idx":22},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ada"},"model_str":"\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.relu = nn.ReLU()\n        \n        dummy_input = torch.zeros(1, 1, input_size)  # Batch size 1, channel 1, input length\n        with torch.no_grad():\n            dummy_output = self.pool(self.relu(self.conv1(dummy_input)))\n            flattened_size = dummy_output.numel()  # Total number of elements in the flattened output\n        \n        self.fc1 = nn.Linear(flattened_size, 64)\n        self.fc2 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # Add channel dimension\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n","model_idx":23},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9adb"},"model_str":"\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.relu = nn.ReLU()\n        \n        # Dynamically calculate the flattened size after conv and pool\n        dummy_input = torch.zeros(1, 1, input_size)  # Batch size 1, channel 1, input length\n        with torch.no_grad():\n            dummy_output = self.pool(self.relu(self.conv2(self.relu(self.conv1(dummy_input)))))\n            flattened_size = dummy_output.numel()  # Total number of elements in the flattened output\n        \n        self.fc1 = nn.Linear(flattened_size, 64)  # Use the calculated size\n        self.fc2 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # Add channel dimension\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n\n","model_idx":24},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9adc"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.fc2 = nn.Linear(128, 64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n\n","model_idx":25},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9add"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 16)\n        self.bn1 = nn.BatchNorm1d(16)\n        self.fc2 = nn.Linear(16, 64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n\n","model_idx":26},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ade"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.shortcut = nn.Linear(input_size, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = self.shortcut(x)  # Skip connection\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x) + residual  # Add skip connection\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n\n","model_idx":27},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9adf"},"model_str":"\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.fc3 = nn.Linear(256, num_classes)\n        self.dropout = nn.Dropout(0.5)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n\n","model_idx":28},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ae0"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.shortcut = nn.Linear(input_size, 64)  # Residual connection\n        self.fc4 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = self.shortcut(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x) + residual\n        x = self.relu(x)\n        x = self.fc4(x)\n        return x\n\n","model_idx":29},{"_id":{"$oid":"67abc9e4bc3cc60a4a0e9ae1"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 8)  \n        self.fc2 = nn.Linear(8, num_classes) \n        self.relu = nn.ReLU()                \n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)  # Apply ReLU after the first layer\n        x = self.fc2(x)             \n        return x\n\n","model_idx":30},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78127"},"model_str":"\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        layers = []\n        in_features = input_size\n        layers.append(nn.Linear(in_features, 256))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(256, 128))\n        layers.append(nn.Linear(128, 128))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(128, 256))\n        layers.append(nn.Linear(256, num_classes))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n","model_idx":31},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78128"},"model_str":"\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        layers = []\n        in_features = input_size\n        layers.append(nn.Linear(in_features, 64))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(64, 32))\n        layers.append(nn.Linear(32, 64))\n        layers.append(nn.ReLU())\n        layers.append(nn.Linear(64, 128))\n        layers.append(nn.Linear(128, num_classes))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n","model_idx":32},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78129"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size, 64)\n        self.layer2 = ResidualBlock(64, 128)\n        self.layer3 = ResidualBlock(128, 256)\n        self.layer4 = ResidualBlock(256, 512)\n        self.fc = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return self.fc(x)\n        \nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, out_features),\n            nn.ReLU(),\n            nn.Linear(out_features, out_features)\n        )\n        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n\n    def forward(self, x):\n        return nn.ReLU()(self.fc(x) + self.shortcut(x))\n\n","model_idx":33},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7812a"},"model_str":"\nimport torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, out_features),\n            nn.ReLU(),\n            nn.Linear(out_features, out_features)\n        )\n        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n\n    def forward(self, x):\n        return nn.ReLU()(self.fc(x) + self.shortcut(x))\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size, 64)\n        self.layer2 = ResidualBlock(64, 128)\n        self.fc = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return self.fc(x)\n\n","model_idx":34},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7812b"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size, 128)\n        self.layer2 = ResidualBlock(128, 256)\n        self.layer3 = ResidualBlock(256, 512)\n        self.fc = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return self.fc(x)\n\n","model_idx":35},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7812c"},"model_str":"\nclass BottleneckBlock(nn.Module):\n    def __init__(self, in_features, bottleneck_size, out_features):\n        super(BottleneckBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, bottleneck_size),\n            nn.ReLU(),\n            nn.Linear(bottleneck_size, out_features)\n        )\n        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n\n    def forward(self, x):\n        return nn.ReLU()(self.fc(x) + self.shortcut(x))\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = BottleneckBlock(input_size, 32, 128)\n        self.layer2 = BottleneckBlock(128, 64, 256)\n        self.layer3 = BottleneckBlock(256, 128, 512)\n        self.fc = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return self.fc(x)\n\n","model_idx":36},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7812d"},"model_str":"\nclass ResidualDenseBlock(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ResidualDenseBlock, self).__init__()\n        self.fc1 = nn.Linear(in_features, out_features)\n        self.fc2 = nn.Linear(out_features, out_features)\n        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n\n    def forward(self, x):\n        return nn.ReLU()(self.fc2(nn.ReLU()(self.fc1(x))) + self.shortcut(x))\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualDenseBlock(input_size, 128)\n        self.layer2 = ResidualDenseBlock(128, 256)\n        self.fc = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return self.fc(x)\n\n","model_idx":37},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7812e"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size, 128)\n        self.dropout = nn.Dropout(0.5)\n        self.layer2 = ResidualBlock(128, 256)\n        self.fc = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.dropout(x)\n        x = self.layer2(x)\n        return self.fc(x)\n\n","model_idx":38},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7812f"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size, 128)\n        self.norm = nn.LayerNorm(128)\n        self.layer2 = ResidualBlock(128, 256)\n        self.fc = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.norm(self.layer1(x))\n        x = self.layer2(x)\n        return self.fc(x)\n\n","model_idx":39},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78130"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size, 32)\n        self.layer2 = ResidualBlock(32, 64)\n        self.fc = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return self.fc(x)\n\n","model_idx":40},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78131"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer_1 = nn.Linear(input_size, 256)\n        self.layer_3 = nn.Linear(256, 128)\n        self.layer_4 = nn.Linear(128, 64)\n        self.final_layer = nn.Linear(64, num_classes)\n        self.activation = nn.ReLU()\n\n    def forward(self, input_data):\n        out = self.layer_1(input_data)\n        out = self.activation(out)\n        \n        out = self.layer_3(out)\n        out = self.activation(out)\n        \n        out = self.layer_4(out)\n        out = self.final_layer(out)\n        return out\n\n","model_idx":41},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78132"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.fc2 = nn.BatchNorm1d(512)\n        self.fc3 = nn.Linear(512, 256)\n        self.fc4 = nn.LayerNorm(256)\n        self.fc5 = nn.Linear(256, 128)\n        self.fc6 = nn.BatchNorm1d(128)\n        self.fc7 = nn.Linear(128, 64)\n        self.fc8 = nn.Linear(64, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.4)\n\n    def forward(self, x):\n        x = self.relu(self.fc2(self.fc1(x)))\n        x = self.dropout(x)\n        x = self.relu(self.fc4(self.fc3(x)))\n        x = self.dropout(x)\n        x = self.relu(self.fc6(self.fc5(x)))\n        x = self.fc7(x)\n        x = self.fc8(x)\n        return x\n","model_idx":42},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78133"},"model_str":"\nclass Model(nn.Module):  # Model with dropout regularization\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 32)\n        self.fc4 = nn.Linear(32, num_classes)\n        \n        self.activation_function = nn.Tanh()\n\n    def forward(self, data):\n        output = self.fc1(data)\n        output = self.activation_function(output)\n        \n        output = self.fc2(output)\n        output = self.activation_function(output)\n        \n        output = self.activation_function(self.fc3(output))\n\n        output = self.fc4(output)\n        return output\n","model_idx":43},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78134"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        # A random comment here trying to confuse codeBERT\n        self.fc1 = nn.Linear(input_size, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        \n        x = self.fc2(x)\n        x = self.relu(x)\n        \n        x = self.fc3(x)\n        return x\n\n","model_idx":44},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78135"},"model_str":"\nclass Model(nn.Module):  # Model with dropout regularization\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.fc1 = nn.Linear(input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, num_classes)     \n        self.activation_function = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.2)\n    # I will try to finish my C25 lecture today as well. I should not spend all my time on this project\n    def forward(self, data):\n        output = self.fc1(data)\n        output = self.activation_function(output)\n        \n        output = self.fc2(output)\n        output = self.activation_function(output)\n        output = self.dropout(output)\n        \n        output = self.fc3(output)\n        output = self.fc4(output)\n        return output\n","model_idx":45},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78136"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(input_size, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n\n        \n\n\n\n\n\n        \n    def forward(self, x):\n        x1 = self.fc1(x)\n        x2 = self.fc2(x)\n        x = torch.max(x1, x2)\n        x = self.fc3(x)\n        return x\n\n","model_idx":46},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78137"},"model_str":"\nclass Model(nn.Module):  # Defines the main model class\n    def __init__(self, input_size, num_classes):  # Constructor\n        super(Model, self).__init__()  # Call parent constructor\n\n        # Layer Definitions\n        self.hidden_layer1 = nn.Linear(input_size, input_size)  # First fully connected layer\n        self.hidden_layer2 = nn.Linear(input_size, 128)  # Second fully connected layer\n        self.final_layer = nn.Linear(128, num_classes)  # Output layer\n\n        # Activation Function\n        self.activation_fn = nn.ReLU()  # Using ReLU activation\n\n    def forward(self, inputs):  # Forward pass logic\n        # Preserve the residual for skip connection\n        skip_connection = inputs  \n\n        # First Layer\n        result = self.hidden_layer1(inputs)  # Apply first linear layer\n        result = self.activation_fn(result)  # Apply ReLU activation\n        result = result +  skip_connection  # Add residual (skip connection)\n\n        # Second Layer\n        result = self.hidden_layer2(result)  # Apply second linear layer\n        result = self.activation_fn(result)  # Apply ReLU activation\n\n        # Final Output\n        result = self.final_layer(result)  # Apply final layer for output\n        return result  # Return output\n","model_idx":47},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78138"},"model_str":"\nclass Model(nn.Module):  # Changed class name for added confusion\n    def __init__(self, input_size, num_classes):  # Updated parameter names\n        super(Model, self).__init__()  # Superclass initialization\n\n        # Initialize layers with descriptive names\n        self.input_layer = nn.Linear(input_size, 128)  # First fully connected layer\n        self.hidden_layer = nn.Linear(128, 64)  # Hidden layer\n        self.output_layer = nn.Linear(64, num_classes)  # Final layer\n\n        # Activation Functions\n        self.activation_relu = nn.ReLU()  # ReLU activation\n        self.activation_tanh = nn.Tanh()  # Tanh activation\n\n    def forward(self, inputs):  # Updated parameter name\n        # First Layer\n        temp_result = self.input_layer(inputs)  # Linear transformation\n        temp_result = self.activation_relu(temp_result)  # ReLU activation\n\n        # Second Layer\n        temp_result = self.hidden_layer(temp_result)  # Linear transformation\n        temp_result = self.activation_tanh(temp_result)  # Tanh activation\n\n        # Output Layer\n        output = self.output_layer(temp_result)  # Final linear transformation\n\n        return output  # Return final output\n","model_idx":48},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78139"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.layer1 = ResidualBlock(input_size)\n        self.layer2 = ResidualBlock(input_size)\n        self.fc = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return self.fc(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, input_size):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, input_size),\n            nn.ReLU(),\n            nn.Linear(input_size, input_size)\n        )\n\n    def forward(self, x):\n        return torch.relu(self.fc(x) + x)\n\n","model_idx":49},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7813a"},"model_str":"\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.hidden = nn.Linear(input_size, 64)  \n        self.output_layer = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.hidden(x))\n        return self.output_layer(x)\n\n","model_idx":50},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7813b"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\n","model_idx":51},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7813c"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 256)\n        self.fc3 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\n","model_idx":52},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7813d"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.layer1 = nn.Linear(input_size, 128)\n        self.batch_norm = nn.BatchNorm1d(128)\n        self.layer2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.batch_norm(torch.relu(self.layer1(x)))\n        return self.layer2(x)\n\n","model_idx":53},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7813e"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.dense1 = nn.Linear(input_size, 256)\n        self.dense2 = nn.Linear(256, 512)\n        self.drop = nn.Dropout(0.3)\n        self.dense3 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = torch.relu(self.dense1(x))\n        x = self.drop(torch.relu(self.dense2(x)))\n        return self.dense3(x)\n\n","model_idx":54},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7813f"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.l1 = nn.Linear(input_size, 64)\n        self.l2 = nn.Linear(64, 128)\n        self.l3 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = torch.selu(self.l1(x))\n        x = torch.selu(self.l2(x))\n        return self.l3(x)\n\n","model_idx":55},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78140"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.linear1 = nn.Linear(input_size, 128)\n        self.linear2 = nn.Linear(128, 64)\n        self.linear3 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = torch.nn.functional.leaky_relu(self.linear1(x), negative_slope=0.02)\n        x = torch.nn.functional.leaky_relu(self.linear2(x), negative_slope=0.02)\n        return self.linear3(x)\n\n","model_idx":56},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78141"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.fc1 = nn.Linear(input_size, 128)\n        self.norm = nn.LayerNorm(128)\n        self.fc2 = nn.Linear(128, 256)\n        self.fc3 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.norm(torch.relu(self.fc1(x)))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\n","model_idx":57},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78142"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = torch.sigmoid(self.fc1(x))  # Swish activation\n        return self.fc2(x)\n\n","model_idx":58},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78143"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.hidden_layer = nn.Linear(input_size, 128)\n        self.output_layer = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = torch.tanh(self.hidden_layer(x))\n        return self.output_layer(x)\n\n","model_idx":59},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78144"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.l1 = nn.Linear(input_size, 64)\n        self.l2 = nn.Linear(64, 64)\n        self.l3 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        identity = x\n        x = torch.relu(self.l1(x))\n        x = x + self.l2(x)  # Residual connection\n        return self.l3(x)\n\n","model_idx":60},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78145"},"model_str":"\nimport torch\nimport torch.nn as nn\n\n\n# Standard MLP but with extra spaces and unnecessary comments\nclass Model(nn.Module):\n    \n    def __init__( self ,input_size , num_classes ):  \n        super( Model , self ).__init__()  \n        \n        # First hidden layer\n        self.layer_one = nn.Linear( input_size , 128 )  \n\n        # Output layer\n        self.final_output = nn.Linear( 128 , num_classes )  \n        \n\n    def forward( self , x ) :  \n\n        # Apply activation function  \n        x = torch.relu(self.layer_one(x))  \n        \n        # Final output  \n        return self.final_output(x)  \n\n\n","model_idx":61},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78146"},"model_str":"\nfrom torch import nn\nimport torch\nclass Model ( nn.Module ):    \n\n    def __init__ ( self , input_size , num_classes ):   \n        super( ) . __init__( )    \n\n        self.layer1 = nn.Linear( input_size, 128 )   \n\n        # Intermediate hidden layer   \n        self.layer2 = nn.Linear( 128 , 256 )    \n\n        # Output layer    \n        self.layer3 = nn.Linear( 256 , num_classes )    \n\n        \n    def forward ( self , x ):  \n\n        # Pass input through first layer\n        x = torch . relu ( self.layer1 ( x ) )    \n\n\n        # Apply second layer  \n        x = torch . relu ( self.layer2 ( x ) )    \n\n        # Output layer \n        return self.layer3 ( x )  \n","model_idx":62},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78147"},"model_str":"\nclass Model(nn.Module):\n    \n    \n    def __init__(self, input_size, num_classes):\n        \n        \n        super(Model, self).__init__()\n        \n        self.hidden = nn.Linear(input_size, 64)  \n        \n        self.final_layer = nn.Linear(64, num_classes)  \n        \n\n\n    def forward(self, x):  \n        \n        # Apply first layer activation\n        x = torch.relu(\n            self.hidden(x)\n        )  \n        \n        \n        return self.final_layer(x)  \n\n","model_idx":63},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78148"},"model_str":"\nclass Model(nn.Module):\n    \n    def __init__(self, input_size, num_classes):\n\n        super(Model, self).__init__()\n\n        self.hidden_layer = nn.Linear(input_size, 64)\n        \n        # Output layer\n        self.output_layer = nn.Linear(64, num_classes)\n\n        \n    def forward(self, x):\n\n        x = torch.relu(\n                (self.hidden_layer(x))\n            )  \n\n        \n        return (\n            self.output_layer(x)\n        )\n\n","model_idx":64},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78149"},"model_str":"\nclass Model(nn.Module):\n\n    def __init__( self, input_size, num_classes ) :\n\n        super(Model, self).__init__()\n\n        # First fully connected layer\n        self.dense1 = nn.Linear(input_size, 128)\n        \n\n        self.dense2 = nn.Linear(128, num_classes)\n\n        \n    # Forward pass\n    def forward( self, x ) :\n\n        x = torch.relu( self.dense1( x ) ) \n\n        return self.dense2( x )  \n\n","model_idx":65},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7814a"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.layer1 = nn.Linear( input_size, 128 ) \n        self.layer2 = nn.Linear( 128, 256 ) \n        self.layer3 = nn.Linear( 256, num_classes ) \n\n\n    def forward(self, x):\n        \n        x = torch.relu(\n                    self.layer1(x)\n                )  \n        \n        x = torch.relu(  \n            (self.layer2(x))  \n        )  \n\n        return (\n            self.layer3( x )  \n        )  \n\n","model_idx":66},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7814b"},"model_str":"\nclass Model(nn.Module):\n    \n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        # Hidden layer\n        self.fc1 = nn.Linear(input_size, 128)\n\n        # Normalization layer\n        self.norm = nn.LayerNorm(128)\n\n        self.fc2 = nn.Linear(128, num_classes)\n\n\n    def forward(self, x):\n\n        # Apply first layer + normalization\n        x = self.norm( torch.relu( self.fc1(x) ) )\n\n        return self.fc2( x )\n\n","model_idx":67},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7814c"},"model_str":"\nclass Model(nn.Module):\n\n    \n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        \n        self.hidden = nn.Linear(input_size, 64)\n        \n        self.output_layer = nn.Linear(64, num_classes)\n        \n\n    \n    def forward(self, x):\n\n        x = torch.relu( self.hidden( x ) )\n\n        return self.output_layer(x)\n\n","model_idx":68},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7814d"},"model_str":"\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.l1 = nn.Linear(input_size, 64)\n        self.l2 = nn.Linear(64, 64)\n        self.l3 = nn.Linear(64, num_classes)\n\n\n    def forward(self, x):\n\n        identity = x  \n        \n        x = torch.relu( self.l1( x ) )  \n\n        x = (  \n            x + self.l2( x )  \n        )  \n\n        return self.l3(x)\n\n","model_idx":69},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7814e"},"model_str":"\nclass Model(nn.Module):\n    \n    \n\n    def __init__(self, input_size, num_classes):\n\n        super(Model, self).__init__()\n\n        self.l1 = nn.Linear(input_size, 64)\n        self.l2 = nn.Linear(64, 128)\n        self.l3 = nn.Linear(128, num_classes)\n\n\n    def forward(self, x):\n\n\n        x = torch.selu(\n            self.l1( x )\n        )\n\n        \n        x = torch.selu( self.l2( x ) )  \n\n        return self.l3(x)\n\n","model_idx":70},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7814f"},"model_str":"\nclass Model(nn.Module):\n\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.layer1 = nn.Linear(input_size, 64)\n        \n        self.layer2 = nn.Linear(64, num_classes)\n\n\n    \n    def forward(self, x):\n\n        with torch.no_grad():  # Unnecessary but still functional\n            temp = x\n\n        x = torch.relu(self.layer1(temp))\n\n        return self.layer2(x)\n\n","model_idx":71},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78150"},"model_str":"\nclass Model(nn.Module):\n\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.layer1 = nn.Linear(input_size, 128)\n        \n        self.layer2 = nn.Linear(128, num_classes)\n\n\n    \n    def forward(self, x):\n\n        with torch.no_grad():  # Unnecessary but still functional\n            temp = x\n\n        x = torch.relu(self.layer1(temp))\n\n        return self.layer2(x)\n\n","model_idx":72},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78151"},"model_str":"\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.hidden1 = nn.Linear(input_size, 64)\n        self.hidden2 = nn.Linear(64, 128)\n        self.hidden3 = nn.Linear(128, 256)  # Never used\n\n        self.final_layer = nn.Linear(64, num_classes)\n\n\n    def forward(self, x):\n\n        x = torch.relu(self.hidden1(x))\n\n        return self.final_layer(x)\n\n","model_idx":73},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78152"},"model_str":"\n\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.l1 = nn.Linear(input_size, input_size)\n        self.l2 = nn.Linear(input_size, num_classes)\n\n    \n    def forward(self, x):\n\n        x = self.l1(x)  \n        x = torch.relu(x)  \n\n        x = self.l1(x)  # Unnecessary reuse of `l1`\n\n        return torch.relu(self.l2(x))\n","model_idx":74},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78153"},"model_str":"\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    \n    def forward(self, x):\n\n        x = torch.relu(self.fc1(x))\n\n        x = sum([self.fc2(x) for _ in range(1)])  # Pointless list comprehension\n\n        return x\n\n","model_idx":75},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78154"},"model_str":"\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.layer1 = nn.Linear(input_size, 256)\n        self.identity = nn.Identity()  # Does absolutely nothing\n        self.output_layer = nn.Linear(256, num_classes)\n\n\n    def forward(self, x):\n\n        x = torch.relu(self.layer1(x))\n\n        x = self.identity(x)  # Identity layer doesn’t affect anything\n\n        return self.output_layer(x)\n\n","model_idx":76},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78155"},"model_str":"\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # Ensure the input has (batch_size, channels, sequence_length)\n        x = torch.relu(self.conv1(x))\n        x = x.mean(dim=-1)  # Global Average Pooling\n        return self.fc(x)\n\n\n","model_idx":77},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78156"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n","model_idx":78},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78157"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n        self.fc = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n","model_idx":79},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78158"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool1d(kernel_size=2)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = self.pool(x)\n        x = torch.relu(self.conv2(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n","model_idx":80},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78159"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm1d(32)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = torch.relu(self.bn2(self.conv2(x)))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n","model_idx":81},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7815a"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = self.dropout(x.mean(dim=-1))\n        return self.fc(x)\n\n","model_idx":82},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7815b"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self,input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n","model_idx":83},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7815c"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n        self.fc = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        identity = x\n        x = torch.relu(self.conv1(x))\n        x = self.conv2(x) + identity  # Residual connection\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n","model_idx":84},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7815d"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n        self.fc = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        identity = x\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x)) + identity\n        x = self.conv3(x)\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n","model_idx":85},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7815e"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.norm = nn.LayerNorm(64)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))  # Shape: (batch_size, 64, seq_length)\n        x = torch.relu(self.conv2(x))             # Shape: (batch_size, 32, seq_length)\n        x = x.mean(dim=-1)                       # Global average pooling\n        return self.fc(x)\n\n\n","model_idx":86},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7815f"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=2, dilation=2)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=2, dilation=2)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n","model_idx":87},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78160"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=4, dilation=4)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=4, dilation=4)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n","model_idx":88},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78161"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1) \n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n","model_idx":89},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78162"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(128, 64, kernel_size=3, padding=1, groups=4)\n        self.conv3 = nn.Conv1d(64, 32, kernel_size=3, padding=1, groups=4)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n","model_idx":90},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78163"},"model_str":"\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=2, dilation=2)\n        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=2, dilation=2, groups=2)\n        self.fc = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = x.mean(dim=-1)\n        return self.fc(x)\n\n","model_idx":91},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78164"},"model_str":"\nclass Model ( nn.Module ) :  \n      \n    def __init__ ( self , input_size , num_classes ):   \n        super( Model , self ) . __init__ ( )    \n\n        # First Convolution Layer\n        self.conv1 = nn.Conv1d( 1 , 64 , kernel_size = 3 , padding = 1 )  \n\n\n        # Second Convolution Layer\n        self.conv2 = nn.Conv1d( 64 , 32 , kernel_size = 3 , padding = 1 )    \n\n        self.fc = nn.Linear ( 32 , num_classes )    \n\n        \n    def forward ( self , x ) :  \n        x = x.unsqueeze(1)\n        x = torch . relu ( ( self.conv1 ( x ) ) )    \n\n\n        x = torch . relu ( ( self.conv2 ( x ) ) )    \n\n        return ( ( self.fc ( x.mean ( dim = -1 ) ) ) )  \n\n","model_idx":92},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78165"},"model_str":"\nclass Model(nn.Module):\n\n\n\n    def __init__(self, input_size, num_classes):\n\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n\n\n        self.fc = nn.Linear(32, num_classes)\n\n\n    \n    def forward(self, x):\n\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n\n        temp_var = x  # Temporary variable that is unnecessary\n\n        x = torch.relu(self.conv2(temp_var))\n\n        return self.fc(x.mean(dim=-1))\n\n","model_idx":93},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78166"},"model_str":"\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.layer_one = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n        self.layer_two = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n        self.layer_three = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n\n        self.fc = nn.Linear(32, num_classes)\n\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(\n                (self.layer_one(x))\n        )  \n\n        x = torch.relu(\n                (self.layer_two(x))\n        )  \n\n        x = torch.relu(\n                (self.layer_three(x))\n        )  \n\n        return self.fc(\n            x.mean(dim=-1)\n        )\n\n","model_idx":94},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78167"},"model_str":"\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n\n        self.fc = nn.Linear(64, num_classes)\n\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = torch.relu(self.conv1(x))\n\n\n        # Useless conditional statement that does nothing\n        if True:\n            x = torch.relu(self.conv2(x))\n\n        return self.fc(x.mean(dim=-1))\n\n","model_idx":95},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78168"},"model_str":"\nclass Model(nn.Module):\n\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n\n        self.fc = nn.Linear(32, num_classes)\n\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        with torch.no_grad():  # Doesn't serve any purpose here\n            temp_x = x\n\n        x = torch.relu(self.conv1(temp_x))\n\n        x = torch.relu(self.conv2(x))\n\n        return self.fc(x.mean(dim=-1))\n\n","model_idx":96},{"_id":{"$oid":"67b7adfd95bd2f5e5ee78169"},"model_str":"\nimport torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, input_size):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, input_size),\n            nn.ReLU(),\n            nn.Linear(input_size, input_size)\n        )\n\n    def forward(self, x):\n        return torch.relu(self.fc(x) + x)\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.block1 = ResidualBlock(input_size)\n        self.fc = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        return self.fc(x)\n\n","model_idx":97},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7816a"},"model_str":"\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.block1 = ResidualBlock(input_size)\n        self.block2 = ResidualBlock(input_size)\n        self.fc = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        return self.fc(x)\n\n        import torch\n        \nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, in_features),\n            nn.ReLU(),\n            nn.Linear(in_features, in_features)\n        )\n\n    def forward(self, x):\n        return torch.relu(self.fc(x) + x)\n","model_idx":98},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7816b"},"model_str":"\nimport torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, input_size):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, input_size),\n            nn.ReLU(),\n            nn.Linear(input_size, input_size)\n        )\n\n    def forward(self, x):\n        return torch.relu(self.fc(x) + x)\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.block1 = ResidualBlock(input_size)\n        self.fc1 = nn.Linear(input_size, 256)\n        self.block2 = ResidualBlock(256)\n        self.fc = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.fc1(x)\n        x = self.block2(x)\n        return self.fc(x)\n\n","model_idx":99},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7816c"},"model_str":"\nclass BottleneckBlock(nn.Module):\n    def __init__(self, in_features, bottleneck_size):\n        super(BottleneckBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, bottleneck_size),\n            nn.ReLU(),\n            nn.Linear(bottleneck_size, in_features)\n        )\n\n    def forward(self, x):\n        return torch.relu(self.fc(x) + x)\n\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.block1 = BottleneckBlock(input_size, 64)\n        self.block2 = BottleneckBlock(input_size, 128)\n        self.fc = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        return self.fc(x)\n\n","model_idx":100},{"_id":{"$oid":"67b7adfd95bd2f5e5ee7816d"},"model_str":"\nimport torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, in_features),\n            nn.ReLU(),\n            nn.Linear(in_features, in_features)\n        )\n\n    def forward(self, x):\n        return torch.relu(self.fc(x) + x)\nclass Model(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Model, self).__init__()\n        self.block1 = ResidualBlock(input_size)\n        self.block2 = ResidualBlock(input_size)\n        self.fc = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        x = torch.selu(self.block1(x))\n        x = torch.selu(self.block2(x))\n        return self.fc(x)\n\n","model_idx":101}]
